{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we first extract learners' mind-wandering from the json data. Then we map the data on the timeline. We process the mind-wandering data generated by reporting and questions seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Reading\n",
    "In this step, we read gaze data of Tobii from a tsv file and mind-wandering reports from a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "folderpath_tobiidata = \"../Data_Publish/Data_Tobii\"\n",
    "folderpath_webgazerdata = \"../Data_Publish/Data_Event\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading Mind-Wandering Data\n",
    "We only read some important data columns from the json file.\n",
    "\n",
    "1. Rating\n",
    "2. Bell Rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data_Publish/Data_Event/Anon01.json', '../Data_Publish/Data_Event/Anon02.json', '../Data_Publish/Data_Event/Anon03.json', '../Data_Publish/Data_Event/Anon04.json', '../Data_Publish/Data_Event/Anon05.json', '../Data_Publish/Data_Event/Anon06.json', '../Data_Publish/Data_Event/Anon07.json', '../Data_Publish/Data_Event/Anon08.json', '../Data_Publish/Data_Event/Anon09.json', '../Data_Publish/Data_Event/Anon10.json', '../Data_Publish/Data_Event/Anon11.json', '../Data_Publish/Data_Event/Anon12.json', '../Data_Publish/Data_Event/Anon13.json']\n",
      "['Anon01', 'Anon02', 'Anon03', 'Anon04', 'Anon05', 'Anon06', 'Anon07', 'Anon08', 'Anon09', 'Anon10', 'Anon11', 'Anon12', 'Anon13']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "list_filepath_report = []\n",
    "list_id_report = []\n",
    "for file in os.listdir(folderpath_webgazerdata):\n",
    "    if file.endswith(\".json\"):\n",
    "        list_filepath_report.append(os.path.join(folderpath_webgazerdata, file))\n",
    "        list_id_report.append(file[0:-5])\n",
    "        \n",
    "print list_filepath_report\n",
    "print list_id_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Format \n",
    "id, starttime_iso, endtime_iso, starttime_video, endtime_video, video_length, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anon01\n",
      "[]\n",
      "Anon02\n",
      "[{'exit': u'2017-04-12T11:28:33.178Z', 'enter': u'2017-04-12T11:20:41.610Z'}, {'exit': u'2017-04-12T11:40:20.780Z', 'enter': u'2017-04-12T11:33:36.025Z'}]\n",
      "Anon03\n",
      "[]\n",
      "Anon04\n",
      "[{'exit': u'2017-04-06T11:53:39.676Z', 'enter': u'2017-04-06T11:45:46.491Z'}, {'exit': u'2017-04-06T12:05:42.023Z', 'enter': u'2017-04-06T11:58:57.059Z'}]\n",
      "Anon05\n",
      "[]\n",
      "Anon06\n",
      "[]\n",
      "Anon07\n",
      "[{'exit': u'2017-04-13T12:45:32.365Z', 'enter': u'2017-04-13T12:38:52.824Z'}, {'exit': u'2017-04-13T12:59:46.088Z', 'enter': u'2017-04-13T12:51:58.970Z'}]\n",
      "Anon08\n",
      "[]\n",
      "Anon09\n",
      "[]\n",
      "Anon10\n",
      "[]\n",
      "Anon11\n",
      "[]\n",
      "Anon12\n",
      "[{'exit': u'2017-04-12T15:37:00.416Z', 'enter': u'2017-04-12T15:30:23.180Z'}, {'exit': u'2017-04-12T15:48:18.222Z', 'enter': u'2017-04-12T15:40:29.354Z'}]\n",
      "Anon13\n",
      "[{'exit': u'2017-04-12T14:32:31.031Z', 'enter': u'2017-04-12T14:24:43.184Z'}, {'exit': u'2017-04-12T14:43:27.772Z', 'enter': u'2017-04-12T14:36:46.908Z'}]\n",
      "                endtime_iso       endtime_video  fullscreen      id  label  \\\n",
      "0  2017-04-12T09:28:47.772Z           38.540809         0.0  Anon01    0.0   \n",
      "1  2017-04-12T09:29:34.224Z    85.0593229256134         0.0  Anon01    0.0   \n",
      "2  2017-04-12T09:30:35.276Z  146.13896398283387         0.0  Anon01    0.0   \n",
      "3  2017-04-12T09:31:36.727Z          206.331911         0.0  Anon01    0.0   \n",
      "4  2017-04-12T09:32:33.103Z  263.93422694659426         0.0  Anon01    0.0   \n",
      "5  2017-04-12T09:33:34.025Z           323.57275         0.0  Anon01    1.0   \n",
      "6  2017-04-12T09:34:11.537Z          362.089951         0.0  Anon01    0.0   \n",
      "7  2017-04-12T09:40:29.560Z           41.821216         0.0  Anon01    0.0   \n",
      "8  2017-04-12T09:41:28.162Z  100.62706982833862         0.0  Anon01    0.0   \n",
      "9  2017-04-12T09:42:27.429Z  159.88924984931947         0.0  Anon01    0.0   \n",
      "\n",
      "              starttime_iso starttime_video video_id       video_length  \\\n",
      "0  2017-04-12T09:28:17.772Z        8.540809  Nuclear            400.921   \n",
      "1  2017-04-12T09:29:04.224Z   55.0593229256  Nuclear            400.921   \n",
      "2  2017-04-12T09:30:05.276Z   116.138963983  Nuclear            400.921   \n",
      "3  2017-04-12T09:31:06.727Z      176.331911  Nuclear            400.921   \n",
      "4  2017-04-12T09:32:03.103Z   233.934226947  Nuclear            400.921   \n",
      "5  2017-04-12T09:33:04.025Z       293.57275  Nuclear            400.921   \n",
      "6  2017-04-12T09:33:41.537Z      332.089951  Nuclear            400.921   \n",
      "7  2017-04-12T09:39:59.560Z       11.821216    Solar  468.0214058956916   \n",
      "8  2017-04-12T09:40:58.162Z   70.6270698283    Solar  468.0214058956916   \n",
      "9  2017-04-12T09:41:57.429Z   129.889249849    Solar  468.0214058956916   \n",
      "\n",
      "   video_order  \n",
      "0          1.0  \n",
      "1          1.0  \n",
      "2          1.0  \n",
      "3          1.0  \n",
      "4          1.0  \n",
      "5          1.0  \n",
      "6          1.0  \n",
      "7          2.0  \n",
      "8          2.0  \n",
      "9          2.0  \n",
      "(200, 10)\n",
      "(58, 10)\n",
      "(142, 10)\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.\n",
      "  2.  2.  2.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  1.\n",
      "  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.\n",
      "  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.\n",
      "  2.  2.]\n",
      "['Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear' 'Nuclear' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar' 'Solar'\n",
      " 'Solar' 'Solar' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear' 'Nuclear'\n",
      " 'Nuclear' 'Nuclear']\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "df_reports = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(list_id_report)):\n",
    "    # id\n",
    "    id_str = list_id_report[i]\n",
    "    print id_str\n",
    "    # starttime_iso, endtime_iso, starttime_video, endtime_video, video_length, label\n",
    "    with open(list_filepath_report[i]) as file_json_data:\n",
    "        json_data = json.load(file_json_data)\n",
    "        # print(json_data['activity'])\n",
    "        \n",
    "        # TODO: full screen playing info\n",
    "        fullscreen_list = []\n",
    "        fullscreen_temp = {'enter': '', 'exit': ''}\n",
    "        for videostatus in json_data['user']['videostatus']:\n",
    "            if videostatus['status'] == 'Fullscreen_enter':\n",
    "                fullscreen_temp = {'enter': '', 'exit': ''}\n",
    "                fullscreen_temp['enter'] = videostatus['time']\n",
    "            elif videostatus['status'] == 'Fullscreen_exit':\n",
    "                if (fullscreen_temp['enter'] != ''):\n",
    "                    fullscreen_temp['exit'] = videostatus['time']\n",
    "                    fullscreen_list.append(fullscreen_temp)\n",
    "                    fullscreen_temp = {'enter': '', 'exit': ''}\n",
    "            elif videostatus['status'] == 'ENDED':\n",
    "                if (fullscreen_temp['enter'] != ''):\n",
    "                    fullscreen_temp['exit'] = videostatus['time']\n",
    "                    fullscreen_list.append(fullscreen_temp)\n",
    "                    fullscreen_temp = {'enter': '', 'exit': ''}           \n",
    "        \n",
    "        print fullscreen_list\n",
    "        \n",
    "        pre_video_id = \"\"\n",
    "        video_order = 1\n",
    "        \n",
    "        for bell in json_data['user']['ratingbells']:\n",
    "            ## End time is the time when bell rings\n",
    "            endtime_iso = bell['time']\n",
    "            # print endtime_iso\n",
    "            endtime_iso_datetime = datetime.datetime.strptime(endtime_iso, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            ## Start time is 30 sec before the end time\n",
    "            starttime_iso_datetime = endtime_iso_datetime - datetime.timedelta(seconds=30)\n",
    "            (dt, micro) = starttime_iso_datetime.strftime('%Y-%m-%dT%H:%M:%S.%f').split('.')\n",
    "            starttime_iso = \"%s.%03dZ\" % (dt, int(micro) / 1000)\n",
    "            # print starttime_iso\n",
    "            \n",
    "            endtime_video = bell['videoTime']\n",
    "            # print endtime_video\n",
    "            ## There is no stop in last 30 sec before the bell rings. \n",
    "            ## Since each time the video playing starts, they will ring the bell after 30 sec.\n",
    "            starttime_video = str(float(endtime_video) - 30)\n",
    "            # print starttime_video\n",
    "            \n",
    "            video_length = bell['videoDuration']\n",
    "            # print video_length\n",
    "\n",
    "            video_id = \"\"           \n",
    "            if float(video_length) < 420:\n",
    "                video_id = \"Nuclear\"  \n",
    "            else:\n",
    "                video_id = \"Solar\"\n",
    "            \n",
    "            if pre_video_id == \"\":\n",
    "                video_order = 1\n",
    "                pre_video_id = video_id\n",
    "            elif video_id != pre_video_id:\n",
    "                video_order = video_order + 1\n",
    "                pre_video_id = video_id\n",
    "            \n",
    "            label = 0\n",
    "            for rating in json_data['user']['ratings']:\n",
    "                ratingtime_iso = rating['time']\n",
    "                # print ratingtime_iso\n",
    "                ratingtime_iso_datetime = datetime.datetime.strptime(ratingtime_iso, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                if ratingtime_iso_datetime > endtime_iso_datetime and ratingtime_iso_datetime < endtime_iso_datetime + datetime.timedelta(seconds=10):\n",
    "                    label = 1\n",
    "                    break\n",
    "            # print label          \n",
    "            \n",
    "            fullscreen_flag = 0\n",
    "            for fullscreen_play in fullscreen_list:\n",
    "                if (starttime_iso > fullscreen_play['enter']) and (endtime_iso < fullscreen_play['exit']):\n",
    "                    fullscreen_flag = 1\n",
    "                    break\n",
    "            \n",
    "            ## Add data into dataframe\n",
    "            df_reports = df_reports.append({'id': id_str,\n",
    "                                            'video_id': video_id,\n",
    "                                            'video_order': video_order,\n",
    "                                            'starttime_iso': starttime_iso, \n",
    "                                            'endtime_iso': endtime_iso,\n",
    "                                            'starttime_video': starttime_video,\n",
    "                                            'endtime_video': endtime_video,\n",
    "                                            'video_length': video_length,\n",
    "                                            'label': label,\n",
    "                                            'fullscreen': fullscreen_flag\n",
    "                                           }, \n",
    "                                           ignore_index=True)\n",
    "print df_reports.head(10)\n",
    "print df_reports.shape\n",
    "print df_reports[df_reports['label'] == 1].shape\n",
    "print df_reports[df_reports['label'] == 0].shape\n",
    "print df_reports.video_order.values\n",
    "print df_reports.video_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading Tobii Gaze Data\n",
    "We only read some important data columns from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data convert funtion\n",
    "\n",
    "import pytz, datetime\n",
    "\n",
    "def localtime_to_utc(localdate, localtimestamp):\n",
    "    # local data format 3/15/2017 needs to be changed\n",
    "    if (len(localdate.split(\"/\")[0]) == 1):\n",
    "        localdate = \"0\" + localdate\n",
    "    timestring = localdate + \" \" + localtimestamp\n",
    "    # print timestring\n",
    "    local = pytz.timezone ('Europe/Amsterdam')\n",
    "    naive = datetime.datetime.strptime(timestring, \"%m/%d/%Y %H:%M:%S.%f\")\n",
    "    local_dt = local.localize(naive, is_dst=None)\n",
    "    utc_dt = local_dt.astimezone(pytz.utc)\n",
    "    # print utc_dt\n",
    "    (dt, micro) = utc_dt.strftime('%Y-%m-%dT%H:%M:%S.%f').split('.')\n",
    "    dt = \"%s.%03dZ\" % (dt, int(micro) / 1000)\n",
    "    # print dt\n",
    "    return dt\n",
    "\n",
    "# # test\n",
    "# localdate = '3/16/2017'\n",
    "# localtimestamp = '10:41:51.388'\n",
    "# utc_dt = localtime_to_utc(localdate,localtimestamp)\n",
    "# print utc_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 61)\n"
     ]
    }
   ],
   "source": [
    "# Data format: id, starttime_iso, endtime_iso, feature 1, feature 2.......\n",
    "import math\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "\n",
    "df_features = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(list_id_report)):\n",
    "    # id\n",
    "    id_str = list_id_report[i]\n",
    "    df_reports_withid = df_reports.loc[df_reports['id'] == id_str]\n",
    "    \n",
    "    ## Read the tsv file based on id_str\n",
    "    path_gazedata_Tobii = os.path.join(folderpath_tobiidata, id_str + \".csv\")\n",
    "    df_GazeData_Tobii = DataFrame.from_csv(path_gazedata_Tobii, sep=\",\")\n",
    "    # print df_GazeData_Tobii.head(5)\n",
    "    df_GazeData_Tobii = df_GazeData_Tobii.reset_index()\n",
    "    \n",
    "#     # print df_GazeData_Tobii.head(5)\n",
    "#     ## Time convert\n",
    "#     df_GazeData_Tobii['Timestamp_utc'] = df_GazeData_Tobii.apply(lambda row: localtime_to_utc(row['RecordingDate'], row['LocalTimeStamp']), axis=1)\n",
    "#     df_GazeData_Tobii = df_GazeData_Tobii.drop('RecordingDate', 1)\n",
    "#     df_GazeData_Tobii = df_GazeData_Tobii.drop('LocalTimeStamp', 1)\n",
    "#     # print df_GazeData_Tobii.head(5)\n",
    "    \n",
    "#     ## Remove unnessesary data   \n",
    "#     df_GazeData_Tobii = df_GazeData_Tobii[['Timestamp_utc',\n",
    "#                                            'FixationIndex',\n",
    "#                                            'GazeEventDuration',\n",
    "#                                            'FixationPointX (MCSpx)',\n",
    "#                                            'FixationPointY (MCSpx)',\n",
    "#                                            'AbsoluteSaccadicDirection',\n",
    "#                                            'GazePointX (ADCSpx)',\n",
    "#                                            'GazePointY (ADCSpx)']]\n",
    "\n",
    "#     print df_GazeData_Tobii.head(10)\n",
    "    \n",
    "    for index, row in df_reports_withid.iterrows():\n",
    "        starttime_iso = row['starttime_iso']\n",
    "#         print starttime_iso\n",
    "        endtime_iso = row['endtime_iso']\n",
    "#         print endtime_iso\n",
    "        \n",
    "        ## Select Data from df_GazeData_Tobii based on starttime_iso and endtime_iso\n",
    "        df_GazeData_Tobii_selected = df_GazeData_Tobii.loc[((df_GazeData_Tobii['Timestamp_utc'] >= starttime_iso) &\n",
    "                                                           (df_GazeData_Tobii['Timestamp_utc'] <= endtime_iso))\n",
    "                                                          ]\n",
    "        \n",
    "        # print df_GazeData_Tobii_selected.head(20)\n",
    "        # print df_GazeData_Tobii_selected.shape\n",
    "        # print df_GazeData_Tobii_selected.columns\n",
    "        \n",
    "        ## Global Features: Feature Selection based on selected data\n",
    "        temp_fixationindex = 0\n",
    "        temp_timestamp = \"\"\n",
    "        temp_FixationPointX = 0\n",
    "        temp_FixationPointY = 0\n",
    "        list_fixationduration = []\n",
    "        list_saccadeduration = []\n",
    "        list_saccadedistance = []\n",
    "        list_saccadeangel = []\n",
    "        \n",
    "        ## Local Features: Feature Selection based on selected data\n",
    "        \n",
    "        # TODO: get the info about the video and fullscreen playing\n",
    "        video_length = row['video_length']\n",
    "        fullscreen_flag = row['fullscreen']\n",
    "        \n",
    "        face_topleft_x = 0\n",
    "        face_topleft_y = 0\n",
    "        face_bottomright_x = 0\n",
    "        face_bottomright_y = 0\n",
    "                \n",
    "        slide_topleft_x = 0\n",
    "        slide_topleft_y = 0\n",
    "        slide_bottomright_x = 0\n",
    "        slide_bottomright_y = 0\n",
    "                \n",
    "        subtitle_topleft_x = 0\n",
    "        subtitle_topleft_y = 0\n",
    "        subtitle_bottomright_x = 0\n",
    "        subtitle_bottomright_y = 0\n",
    "        \n",
    "        # Select coordinate boundaries for the \n",
    "        if video_length <= 410: # Nucl should be replaced by video_id\n",
    "            if fullscreen_flag:\n",
    "                \n",
    "                face_topleft_x = 1191\n",
    "                face_topleft_y = 239\n",
    "                face_bottomright_x = 1191+331\n",
    "                face_bottomright_y = 239+280\n",
    "                \n",
    "                slide_topleft_x = 137\n",
    "                slide_topleft_y = 132\n",
    "                slide_bottomright_x = 137+929\n",
    "                slide_bottomright_y = 132+557\n",
    "                \n",
    "                subtitle_topleft_x = 402\n",
    "                subtitle_topleft_y = 892\n",
    "                subtitle_bottomright_x = 402+1134\n",
    "                subtitle_bottomright_y = 892+134\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                face_topleft_x = 1089\n",
    "                face_topleft_y = 297\n",
    "                face_bottomright_x = 1089+139\n",
    "                face_bottomright_y = 297+141\n",
    "                \n",
    "                slide_topleft_x = 558\n",
    "                slide_topleft_y = 246\n",
    "                slide_bottomright_x = 558+456\n",
    "                slide_bottomright_y = 246+273\n",
    "                \n",
    "                subtitle_topleft_x = 721\n",
    "                subtitle_topleft_y = 617\n",
    "                subtitle_bottomright_x = 721+478\n",
    "                subtitle_bottomright_y = 617+67\n",
    "        \n",
    "        elif video_length > 450: # Solar\n",
    "            \n",
    "            if fullscreen_flag:\n",
    "                \n",
    "                face_topleft_x = 501\n",
    "                face_topleft_y = 189\n",
    "                face_bottomright_x = 501+267\n",
    "                face_bottomright_y = 189+260\n",
    "                \n",
    "                slide_topleft_x = 861\n",
    "                slide_topleft_y = 313\n",
    "                slide_bottomright_x = 861+811\n",
    "                slide_bottomright_y = 313+483\n",
    "                \n",
    "                subtitle_topleft_x = 458\n",
    "                subtitle_topleft_y = 900\n",
    "                subtitle_bottomright_x = 458+1010\n",
    "                subtitle_bottomright_y = 900+128\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                face_topleft_x = 721\n",
    "                face_topleft_y = 263\n",
    "                face_bottomright_x = 721+146\n",
    "                face_bottomright_y = 263+139\n",
    "                \n",
    "                slide_topleft_x = 913\n",
    "                slide_topleft_y = 330\n",
    "                slide_bottomright_x = 913+401\n",
    "                slide_bottomright_y = 330+241\n",
    "                \n",
    "                subtitle_topleft_x = 709\n",
    "                subtitle_topleft_y = 622\n",
    "                subtitle_bottomright_x = 709+499\n",
    "                subtitle_bottomright_y = 622+60\n",
    "        \n",
    "        ## Define a basic funtion for calculating whether fixations in aoi or not.\n",
    "        def isinaoi(fixation_x, fixation_y):            \n",
    "            if ((fixation_x >= face_topleft_x and fixation_x <= face_bottomright_x) and \n",
    "               (fixation_y >= face_topleft_y and fixation_y <= face_bottomright_y)):\n",
    "                return \"face\"\n",
    "            elif ((fixation_x >= subtitle_topleft_x and fixation_x <= subtitle_bottomright_x) and \n",
    "               (fixation_y >= subtitle_topleft_y and fixation_y <= subtitle_bottomright_y)):\n",
    "                return \"subtitle\"\n",
    "            elif ((fixation_x >= slide_topleft_x and fixation_x <= slide_bottomright_x) and \n",
    "               (fixation_y >= slide_topleft_y and fixation_y <= slide_bottomright_y)): \n",
    "                return \"slide\"\n",
    "            else:\n",
    "                return \"out\"\n",
    "        \n",
    "        ## Define local features\n",
    "        \n",
    "        # num of saccade jump from one area to another\n",
    "        num_saccade_aoi_face_out2in = 0\n",
    "        num_saccade_aoi_face_aoi2in = 0\n",
    "        num_saccade_aoi_face_in2out = 0\n",
    "        num_saccade_aoi_face_in2aoi = 0\n",
    "        num_saccade_aoi_face_within = 0\n",
    "        \n",
    "        num_saccade_aoi_slide_out2in = 0\n",
    "        num_saccade_aoi_slide_aoi2in = 0\n",
    "        num_saccade_aoi_slide_in2out = 0\n",
    "        num_saccade_aoi_slide_in2aoi = 0\n",
    "        num_saccade_aoi_slide_within = 0\n",
    "        \n",
    "        num_saccade_aoi_subtitle_out2in = 0\n",
    "        num_saccade_aoi_subtitle_aoi2in = 0\n",
    "        num_saccade_aoi_subtitle_in2out = 0\n",
    "        num_saccade_aoi_subtitle_in2aoi = 0\n",
    "        num_saccade_aoi_subtitle_within = 0\n",
    "        \n",
    "        temp_aoi = \"out\"\n",
    "        \n",
    "        # numbers and durations of fixations in AOIs.\n",
    "        list_duration_fixation_aoi_face = []\n",
    "        list_duration_fixation_aoi_subtitle = []\n",
    "        list_duration_fixation_aoi_slide = []\n",
    "        # fixations out of AOIs\n",
    "        list_duration_fixation_aoi_out = []\n",
    "        \n",
    "        for index, row in df_GazeData_Tobii_selected.iterrows():\n",
    "            if np.isnan(row['FixationIndex']):\n",
    "                continue\n",
    "            \n",
    "            if temp_fixationindex == 0:\n",
    "                temp_fixationindex = row['FixationIndex']\n",
    "                temp_timestamp = row['Timestamp_utc']\n",
    "                temp_FixationPointX = row['FixationPointX (MCSpx)']\n",
    "                temp_FixationPointY = row['FixationPointY (MCSpx)']\n",
    "                \n",
    "                list_fixationduration.append(row['GazeEventDuration'])\n",
    "                list_saccadeangel.append(row['AbsoluteSaccadicDirection'])\n",
    "                \n",
    "                ## calculate local features\n",
    "                current_aoi = isinaoi(row['FixationPointX (MCSpx)'], row['FixationPointY (MCSpx)'])\n",
    "                if current_aoi == \"face\":\n",
    "                    list_duration_fixation_aoi_face.append(row['GazeEventDuration'])\n",
    "                elif current_aoi == \"subtitle\":\n",
    "                    list_duration_fixation_aoi_subtitle.append(row['GazeEventDuration'])\n",
    "                elif current_aoi == \"slide\":\n",
    "                    list_duration_fixation_aoi_slide.append(row['GazeEventDuration'])\n",
    "                else:\n",
    "                    list_duration_fixation_aoi_out.append(row['GazeEventDuration'])                \n",
    "                temp_aoi = current_aoi\n",
    "            \n",
    "            elif temp_fixationindex != row['FixationIndex']:\n",
    "                \n",
    "                # Global features\n",
    "                temp_fixationindex = row['FixationIndex']\n",
    "                list_fixationduration.append(row['GazeEventDuration'])\n",
    "                list_saccadeangel.append(row['AbsoluteSaccadicDirection'])\n",
    "                \n",
    "                datetime_previous = datetime.datetime.strptime(temp_timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                datetime_current = datetime.datetime.strptime(row['Timestamp_utc'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                saccadeduration = datetime_current - datetime_previous\n",
    "                list_saccadeduration.append(float(saccadeduration.total_seconds() * 1000))\n",
    "                \n",
    "                FixationPointX_current = row['FixationPointX (MCSpx)']\n",
    "                FixationPointY_current = row['FixationPointY (MCSpx)']\n",
    "                saccadedistance = math.sqrt(math.pow((FixationPointX_current - temp_FixationPointX), 2) + \n",
    "                                            math.pow((FixationPointY_current - temp_FixationPointY), 2))\n",
    "                list_saccadedistance.append(saccadedistance)\n",
    "                \n",
    "                temp_timestamp = row['Timestamp_utc']\n",
    "                temp_FixationPointX = row['FixationPointX (MCSpx)']\n",
    "                temp_FixationPointY = row['FixationPointY (MCSpx)']\n",
    "                \n",
    "                # Local features\n",
    "                current_aoi = isinaoi(row['FixationPointX (MCSpx)'], row['FixationPointY (MCSpx)'])\n",
    "                \n",
    "                if current_aoi == \"face\":\n",
    "                    \n",
    "                    list_duration_fixation_aoi_face.append(row['GazeEventDuration'])\n",
    "                    \n",
    "                    if temp_aoi == \"face\":\n",
    "                        num_saccade_aoi_face_within = num_saccade_aoi_face_within + 1\n",
    "                    elif temp_aoi == \"out\":\n",
    "                        num_saccade_aoi_face_out2in = num_saccade_aoi_face_out2in + 1\n",
    "                    else:\n",
    "                        num_saccade_aoi_face_aoi2in = num_saccade_aoi_face_aoi2in + 1\n",
    "                        if temp_aoi == \"slide\":\n",
    "                            num_saccade_aoi_slide_in2aoi = num_saccade_aoi_slide_in2aoi + 1\n",
    "                        else:\n",
    "                            num_saccade_aoi_subtitle_in2aoi = num_saccade_aoi_subtitle_in2aoi + 1\n",
    "                        \n",
    "                elif current_aoi == \"subtitle\":\n",
    "                    \n",
    "                    list_duration_fixation_aoi_subtitle.append(row['GazeEventDuration'])\n",
    "                    \n",
    "                    if temp_aoi == \"subtitle\":\n",
    "                        num_saccade_aoi_subtitle_within = num_saccade_aoi_subtitle_within + 1\n",
    "                    elif temp_aoi == \"out\":\n",
    "                        num_saccade_aoi_subtitle_out2in = num_saccade_aoi_subtitle_out2in + 1\n",
    "                    else:\n",
    "                        num_saccade_aoi_subtitle_aoi2in = num_saccade_aoi_subtitle_aoi2in + 1\n",
    "                        if temp_aoi == \"face\":\n",
    "                            num_saccade_aoi_face_in2aoi = num_saccade_aoi_face_in2aoi + 1\n",
    "                        else:\n",
    "                            num_saccade_aoi_slide_in2aoi = num_saccade_aoi_slide_in2aoi + 1\n",
    "                    \n",
    "                elif current_aoi == \"slide\":\n",
    "                    list_duration_fixation_aoi_slide.append(row['GazeEventDuration'])\n",
    "                    \n",
    "                    if temp_aoi == \"slide\":\n",
    "                        num_saccade_aoi_slide_within = num_saccade_aoi_slide_within + 1\n",
    "                    elif temp_aoi == \"out\":\n",
    "                        num_saccade_aoi_slide_out2in = num_saccade_aoi_slide_out2in + 1\n",
    "                    else:\n",
    "                        num_saccade_aoi_subtitle_aoi2in = num_saccade_aoi_subtitle_aoi2in + 1\n",
    "                        if temp_aoi == \"face\":\n",
    "                            num_saccade_aoi_face_in2aoi = num_saccade_aoi_face_in2aoi + 1\n",
    "                        else:\n",
    "                            num_saccade_aoi_subtitle_in2aoi = num_saccade_aoi_subtitle_in2aoi + 1\n",
    "                    \n",
    "                else:\n",
    "                    list_duration_fixation_aoi_out.append(row['GazeEventDuration'])\n",
    "                    if temp_aoi == \"slide\":\n",
    "                        num_saccade_aoi_slide_in2out = num_saccade_aoi_slide_in2out + 1\n",
    "                    elif temp_aoi == \"face\":\n",
    "                        num_saccade_aoi_face_in2out = num_saccade_aoi_face_in2out + 1\n",
    "                    elif temp_aoi == \"subtitle\":\n",
    "                        num_saccade_aoi_subtitle_in2out = num_saccade_aoi_subtitle_in2out + 1\n",
    "                 \n",
    "                temp_aoi = current_aoi\n",
    "        \n",
    "        num_saccade_horizon = sum(1 for i in list_saccadeangel if ((i <= 30 and i >= -30) or (i >= 150 and i <= 210) or (i >= 330)))\n",
    "#         print num_saccade_horizon\n",
    "#         print len(list_fixationduration)\n",
    "#         print len(list_saccadeduration)\n",
    "#         print len(list_saccadedistance)\n",
    "#         print len(list_saccadeangel)\n",
    "#         print len(list_duration_fixation_aoi_face)\n",
    "#         print len(list_duration_fixation_aoi_subtitle)\n",
    "#         print len(list_duration_fixation_aoi_slide)\n",
    "#         print len(list_duration_fixation_aoi_out)\n",
    "        \n",
    "        duration_fixation_aoi_face = 0\n",
    "        duration_fixation_aoi_face_max = 0\n",
    "        duration_fixation_aoi_subtitle = 0\n",
    "        duration_fixation_aoi_subtitle_max = 0\n",
    "        duration_fixation_aoi_slide = 0\n",
    "        duration_fixation_aoi_slide_max = 0\n",
    "        duration_fixation_aoi_out = 0\n",
    "        duration_fixation_aoi_out_max = 0 \n",
    "        \n",
    "        if len(list_duration_fixation_aoi_face) != 0:\n",
    "            duration_fixation_aoi_face = sum(list_duration_fixation_aoi_face)/sum(list_fixationduration)\n",
    "            duration_fixation_aoi_face_max = np.max(list_duration_fixation_aoi_face)\n",
    "        if len(list_duration_fixation_aoi_subtitle) != 0:\n",
    "            duration_fixation_aoi_subtitle = sum(list_duration_fixation_aoi_subtitle)/sum(list_fixationduration)\n",
    "            duration_fixation_aoi_subtitle_max = np.max(list_duration_fixation_aoi_subtitle)\n",
    "        if len(list_duration_fixation_aoi_slide) != 0:\n",
    "            duration_fixation_aoi_slide = sum(list_duration_fixation_aoi_slide)/sum(list_fixationduration)\n",
    "            duration_fixation_aoi_slide_max = np.max(list_duration_fixation_aoi_slide)\n",
    "        if len(list_duration_fixation_aoi_out) != 0:\n",
    "            duration_fixation_aoi_out = sum(list_duration_fixation_aoi_out)/sum(list_fixationduration)\n",
    "            duration_fixation_aoi_out_max = np.max(list_duration_fixation_aoi_out)\n",
    "        \n",
    "        ## Add features into df_features\n",
    "        df_features = df_features.append({\n",
    "                'id': id_str, \n",
    "                'starttime_iso': starttime_iso, \n",
    "                'endtime_iso': endtime_iso,\n",
    "                'fixationduration_min': np.min(list_fixationduration),\n",
    "                'fixationduration_max': np.max(list_fixationduration),\n",
    "                'fixationduration_mean': np.mean(list_fixationduration),\n",
    "                'fixationduration_median': np.median(list_fixationduration),\n",
    "                'fixationduration_stddev': np.std(list_fixationduration),\n",
    "                'fixationduration_range': np.max(list_fixationduration) - np.min(list_fixationduration),\n",
    "                'fixationduration_kurtosis': kurtosis(list_fixationduration),\n",
    "                'fixationduration_skew': skew(list_fixationduration),\n",
    "                'saccadeduration_min': np.min(list_saccadeduration),\n",
    "                'saccadeduration_max': np.max(list_saccadeduration),\n",
    "                'saccadeduration_mean': np.mean(list_saccadeduration),\n",
    "                'saccadeduration_median': np.median(list_saccadeduration),\n",
    "                'saccadeduration_stddev': np.std(list_saccadeduration),\n",
    "                'saccadeduration_range': np.max(list_saccadeduration) - np.min(list_saccadeduration),\n",
    "                'saccadeduration_kurtosis': kurtosis(list_saccadeduration),\n",
    "                'saccadeduration_skew': skew(list_saccadeduration),\n",
    "                'saccadedistance_min': np.min(list_saccadedistance),\n",
    "                'saccadedistance_max': np.max(list_saccadedistance),\n",
    "                'saccadedistance_mean': np.mean(list_saccadedistance),\n",
    "                'saccadedistance_median': np.median(list_saccadedistance),\n",
    "                'saccadedistance_stddev': np.std(list_saccadedistance),\n",
    "                'saccadedistance_range': np.max(list_saccadedistance) - np.min(list_saccadedistance),\n",
    "                'saccadedistance_kurtosis': kurtosis(list_saccadedistance),\n",
    "                'saccadedistance_skew': skew(list_saccadedistance),\n",
    "                'saccadeangel_min': np.min(list_saccadeangel),\n",
    "                'saccadeangel_max': np.max(list_saccadeangel),\n",
    "                'saccadeangel_mean': np.mean(list_saccadeangel),\n",
    "                'saccadeangel_median': np.median(list_saccadeangel),\n",
    "                'saccadeangel_stddev': np.std(list_saccadeangel),\n",
    "                'saccadeangel_range': np.max(list_saccadeangel) - np.min(list_saccadeangel),\n",
    "                'saccadeangel_kurtosis': kurtosis(list_saccadeangel),\n",
    "                'saccadeangel_skew': skew(list_saccadeangel),\n",
    "                'saccade_num': len(list_saccadeduration),\n",
    "                'saccade_horizonratio': num_saccade_horizon/len(list_saccadeangel),\n",
    "                'fixation_saccade_ratio': sum(list_fixationduration)/sum(list_saccadeduration),\n",
    "                ## LOCAL FEATURES\n",
    "                'num_saccade_aoi_face_out2in': num_saccade_aoi_face_out2in/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_face_aoi2in': num_saccade_aoi_face_aoi2in/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_face_in2out': num_saccade_aoi_face_in2out/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_face_in2aoi': num_saccade_aoi_face_in2aoi/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_face_within': num_saccade_aoi_face_within/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_slide_out2in': num_saccade_aoi_slide_out2in/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_slide_aoi2in': num_saccade_aoi_slide_aoi2in/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_slide_in2out': num_saccade_aoi_slide_in2out/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_slide_in2aoi': num_saccade_aoi_slide_in2aoi/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_slide_within': num_saccade_aoi_slide_within/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_subtitle_out2in': num_saccade_aoi_subtitle_out2in/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_subtitle_aoi2in': num_saccade_aoi_subtitle_aoi2in/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_subtitle_in2out': num_saccade_aoi_subtitle_in2out/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_subtitle_in2aoi': num_saccade_aoi_subtitle_in2aoi/len(list_saccadeduration),\n",
    "                'num_saccade_aoi_subtitle_within': num_saccade_aoi_subtitle_within/len(list_saccadeduration),\n",
    "                'duration_fixation_aoi_face': duration_fixation_aoi_face,\n",
    "                'duration_fixation_aoi_face_max': duration_fixation_aoi_face_max,\n",
    "                'duration_fixation_aoi_subtitle': duration_fixation_aoi_subtitle,\n",
    "                'duration_fixation_aoi_subtitle_max': duration_fixation_aoi_subtitle_max,\n",
    "                'duration_fixation_aoi_slide': duration_fixation_aoi_slide,\n",
    "                'duration_fixation_aoi_slide_max': duration_fixation_aoi_slide_max,\n",
    "                'duration_fixation_aoi_out': duration_fixation_aoi_out,\n",
    "                'duration_fixation_aoi_out_max': duration_fixation_aoi_out_max\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "        # print df_features.head(1)\n",
    "print df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Merge features with labels\n",
    "df_merge = pd.merge(df_reports, df_features)\n",
    "df_merge.to_csv(\"features_labels_tobii.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'endtime_iso', u'endtime_video', u'fullscreen', u'id', u'label',\n",
      "       u'starttime_iso', u'starttime_video', u'video_id', u'video_length',\n",
      "       u'video_order', u'duration_fixation_aoi_face',\n",
      "       u'duration_fixation_aoi_face_max', u'duration_fixation_aoi_out',\n",
      "       u'duration_fixation_aoi_out_max', u'duration_fixation_aoi_slide',\n",
      "       u'duration_fixation_aoi_slide_max', u'duration_fixation_aoi_subtitle',\n",
      "       u'duration_fixation_aoi_subtitle_max', u'fixation_saccade_ratio',\n",
      "       u'fixationduration_kurtosis', u'fixationduration_max',\n",
      "       u'fixationduration_mean', u'fixationduration_median',\n",
      "       u'fixationduration_min', u'fixationduration_range',\n",
      "       u'fixationduration_skew', u'fixationduration_stddev',\n",
      "       u'num_saccade_aoi_face_aoi2in', u'num_saccade_aoi_face_in2aoi',\n",
      "       u'num_saccade_aoi_face_in2out', u'num_saccade_aoi_face_out2in',\n",
      "       u'num_saccade_aoi_face_within', u'num_saccade_aoi_slide_aoi2in',\n",
      "       u'num_saccade_aoi_slide_in2aoi', u'num_saccade_aoi_slide_in2out',\n",
      "       u'num_saccade_aoi_slide_out2in', u'num_saccade_aoi_slide_within',\n",
      "       u'num_saccade_aoi_subtitle_aoi2in', u'num_saccade_aoi_subtitle_in2aoi',\n",
      "       u'num_saccade_aoi_subtitle_in2out', u'num_saccade_aoi_subtitle_out2in',\n",
      "       u'num_saccade_aoi_subtitle_within', u'saccade_horizonratio',\n",
      "       u'saccade_num', u'saccadeangel_kurtosis', u'saccadeangel_max',\n",
      "       u'saccadeangel_mean', u'saccadeangel_median', u'saccadeangel_min',\n",
      "       u'saccadeangel_range', u'saccadeangel_skew', u'saccadeangel_stddev',\n",
      "       u'saccadedistance_kurtosis', u'saccadedistance_max',\n",
      "       u'saccadedistance_mean', u'saccadedistance_median',\n",
      "       u'saccadedistance_min', u'saccadedistance_range',\n",
      "       u'saccadedistance_skew', u'saccadedistance_stddev',\n",
      "       u'saccadeduration_kurtosis', u'saccadeduration_max',\n",
      "       u'saccadeduration_mean', u'saccadeduration_median',\n",
      "       u'saccadeduration_min', u'saccadeduration_range',\n",
      "       u'saccadeduration_skew', u'saccadeduration_stddev'],\n",
      "      dtype='object')\n",
      "Index([u'duration_fixation_aoi_face', u'duration_fixation_aoi_face_max',\n",
      "       u'duration_fixation_aoi_out', u'duration_fixation_aoi_out_max',\n",
      "       u'duration_fixation_aoi_slide', u'duration_fixation_aoi_slide_max',\n",
      "       u'duration_fixation_aoi_subtitle',\n",
      "       u'duration_fixation_aoi_subtitle_max'],\n",
      "      dtype='object')\n",
      "Index([u'fixation_saccade_ratio', u'fixationduration_kurtosis',\n",
      "       u'fixationduration_max', u'fixationduration_mean',\n",
      "       u'fixationduration_median', u'fixationduration_min',\n",
      "       u'fixationduration_range', u'fixationduration_skew',\n",
      "       u'fixationduration_stddev'],\n",
      "      dtype='object')\n",
      "Index([u'num_saccade_aoi_face_aoi2in', u'num_saccade_aoi_face_in2aoi',\n",
      "       u'num_saccade_aoi_face_in2out', u'num_saccade_aoi_face_out2in',\n",
      "       u'num_saccade_aoi_face_within', u'num_saccade_aoi_slide_aoi2in',\n",
      "       u'num_saccade_aoi_slide_in2aoi', u'num_saccade_aoi_slide_in2out',\n",
      "       u'num_saccade_aoi_slide_out2in', u'num_saccade_aoi_slide_within',\n",
      "       u'num_saccade_aoi_subtitle_aoi2in', u'num_saccade_aoi_subtitle_in2aoi',\n",
      "       u'num_saccade_aoi_subtitle_in2out', u'num_saccade_aoi_subtitle_out2in',\n",
      "       u'num_saccade_aoi_subtitle_within'],\n",
      "      dtype='object')\n",
      "Index([u'saccade_horizonratio', u'saccade_num', u'saccadeangel_kurtosis',\n",
      "       u'saccadeangel_max', u'saccadeangel_mean', u'saccadeangel_median',\n",
      "       u'saccadeangel_min', u'saccadeangel_range', u'saccadeangel_skew',\n",
      "       u'saccadeangel_stddev', u'saccadedistance_kurtosis',\n",
      "       u'saccadedistance_max', u'saccadedistance_mean',\n",
      "       u'saccadedistance_median', u'saccadedistance_min',\n",
      "       u'saccadedistance_range', u'saccadedistance_skew',\n",
      "       u'saccadedistance_stddev', u'saccadeduration_kurtosis',\n",
      "       u'saccadeduration_max', u'saccadeduration_mean',\n",
      "       u'saccadeduration_median', u'saccadeduration_min',\n",
      "       u'saccadeduration_range', u'saccadeduration_skew',\n",
      "       u'saccadeduration_stddev'],\n",
      "      dtype='object')\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print df_merge.columns\n",
    "print df_merge.columns[10:18]\n",
    "print df_merge.columns[18:27]\n",
    "print df_merge.columns[27:42]\n",
    "print df_merge.columns[42:]\n",
    "print len(df_merge.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction With Global Features and Local Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression\n",
      "\tPrecision: 0.370\n",
      "\tRecall: 0.172\n",
      "\tF1: 0.235\n",
      "\tAccuracy: 0.675\n",
      "\n",
      "L2 Logistic Regression\n",
      "\tPrecision: 0.300\n",
      "\tRecall: 0.155\n",
      "\tF1: 0.205\n",
      "\tAccuracy: 0.650\n",
      "\n",
      "SVC\n",
      "\tPrecision: 0.000\n",
      "\tRecall: 0.000\n",
      "\tF1: 0.000\n",
      "\tAccuracy: 0.710\n",
      "\n",
      "Decision Tree\n",
      "\tPrecision: 0.324\n",
      "\tRecall: 0.379\n",
      "\tF1: 0.349\n",
      "\tAccuracy: 0.590\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\tPrecision: 0.294\n",
      "\tRecall: 0.345\n",
      "\tF1: 0.317\n",
      "\tAccuracy: 0.570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "## Read data from features_labels_tobii.csv\n",
    "\n",
    "# df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "list_id_report = list(df_merge['id'].unique())\n",
    "\n",
    "y_test_total = []\n",
    "y_pred_l1_total = []\n",
    "y_pred_l2_total = []\n",
    "y_pred_svc_total = []\n",
    "y_pred_tree_total = []\n",
    "y_pred_bayes_total = []\n",
    "y_pred_rf_total = []\n",
    "\n",
    "## Leave-one-out machine learning methods (try Logistic Regression first)\n",
    "for i in range(0, len(list_id_report)):\n",
    "    id_str = list_id_report[i]\n",
    "    \n",
    "    # print id_str\n",
    "    # print \"Run: \" + str(i)\n",
    "    data_train = df_merge.ix[df_merge['id'] != id_str]\n",
    "    # print data_train.shape\n",
    "    data_test = df_merge.ix[df_merge['id'] == id_str]\n",
    "    # print data_test.shape\n",
    "    \n",
    "    X_train = data_train.ix[:, 10:].fillna(value=0)\n",
    "    y_train = data_train.ix[:, 4]\n",
    "    \n",
    "#     sm = SMOTE(random_state=48)\n",
    "#     X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    X_test = data_test.ix[:, 10:].fillna(value=0)\n",
    "    y_test = data_test.ix[:, 4]\n",
    "    \n",
    "    y_test_total.extend(y_test)\n",
    "    \n",
    "    ## Logistic Regression\n",
    "    clf_l1_LR = LogisticRegression(penalty='l1', tol=0.01)\n",
    "    clf_l1_LR = clf_l1_LR.fit(X_train, y_train)\n",
    "    y_pred_l1 = clf_l1_LR.predict(X_test)\n",
    "    y_pred_l1_total.extend(y_pred_l1)\n",
    "    \n",
    "    clf_l2_LR = LogisticRegression(penalty='l2', tol=0.01)\n",
    "    clf_l2_LR = clf_l2_LR.fit(X_train, y_train)\n",
    "    y_pred_l2 = clf_l2_LR.predict(X_test)\n",
    "    y_pred_l2_total.extend(y_pred_l2)\n",
    "        \n",
    "    ## SVM with unbalanced class weight\n",
    "    clf_SVC = SVC(tol=0.01)\n",
    "    clf_SVC = clf_SVC.fit(X_train, y_train)\n",
    "    y_pred_svc = clf_SVC.predict(X_test)\n",
    "    y_pred_svc_total.extend(y_pred_svc)\n",
    "    \n",
    "    ## Decision tree\n",
    "    clf_tree = DecisionTreeClassifier()\n",
    "    clf_tree = clf_tree.fit(X_train, y_train)\n",
    "    y_pred_tree = clf_tree.predict(X_test)\n",
    "    y_pred_tree_total.extend(y_pred_tree)\n",
    "    \n",
    "    \n",
    "    ## GaussianNB\n",
    "    clf_GaussianNB = GaussianNB()\n",
    "    clf_GaussianNB = clf_GaussianNB.fit(X_train, y_train)\n",
    "    y_pred_bayes = clf_GaussianNB.predict(X_test)\n",
    "    y_pred_bayes_total.extend(y_pred_bayes)\n",
    "    \n",
    "#     ## Random Forest\n",
    "#     clf_rf = RandomForestClassifier()\n",
    "#     clf_rf = clf_rf.fit(X_train, y_train)\n",
    "#     y_pred_rf = clf_rf.predict(X_test)\n",
    "#     y_pred_rf_total.extend(y_pred_rf)\n",
    "    \n",
    "\n",
    "print \"L1 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l1_total))\n",
    "\n",
    "print \"L2 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l2_total))\n",
    "\n",
    "print \"SVC\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_svc_total))\n",
    "\n",
    "print \"Decision Tree\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_tree_total))\n",
    "\n",
    "print \"Gaussian Naive Bayes\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_bayes_total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction With Local Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression\n",
      "\tPrecision: 0.143\n",
      "\tRecall: 0.017\n",
      "\tF1: 0.031\n",
      "\tAccuracy: 0.685\n",
      "\n",
      "L2 Logistic Regression\n",
      "\tPrecision: 0.000\n",
      "\tRecall: 0.000\n",
      "\tF1: 0.000\n",
      "\tAccuracy: 0.710\n",
      "\n",
      "SVC\n",
      "\tPrecision: 0.000\n",
      "\tRecall: 0.000\n",
      "\tF1: 0.000\n",
      "\tAccuracy: 0.710\n",
      "\n",
      "Decision Tree\n",
      "\tPrecision: 0.355\n",
      "\tRecall: 0.379\n",
      "\tF1: 0.367\n",
      "\tAccuracy: 0.620\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\tPrecision: 0.448\n",
      "\tRecall: 0.224\n",
      "\tF1: 0.299\n",
      "\tAccuracy: 0.695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "## Read data from features_labels_tobii.csv\n",
    "\n",
    "# df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "list_id_report = list(df_merge['id'].unique())\n",
    "\n",
    "y_test_total = []\n",
    "y_pred_l1_total = []\n",
    "y_pred_l2_total = []\n",
    "y_pred_svc_total = []\n",
    "y_pred_tree_total = []\n",
    "y_pred_bayes_total = []\n",
    "y_pred_rf_total = []\n",
    "\n",
    "## Leave-one-out machine learning methods (try Logistic Regression first)\n",
    "for i in range(0, len(list_id_report)):\n",
    "    id_str = list_id_report[i]\n",
    "    \n",
    "    # print id_str\n",
    "    # print \"Run: \" + str(i)\n",
    "    data_train = df_merge.ix[df_merge['id'] != id_str]\n",
    "    # print data_train.shape\n",
    "    data_test = df_merge.ix[df_merge['id'] == id_str]\n",
    "    # print data_test.shape\n",
    "    feature_index_local = [10,11,12,13,14,15,16,17,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]\n",
    "    X_train = data_train.ix[:, feature_index_local].fillna(value=0)\n",
    "    y_train = data_train.ix[:, 4]\n",
    "    \n",
    "#     sm = SMOTE(random_state=48)\n",
    "#     X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    X_test = data_test.ix[:, feature_index_local].fillna(value=0)\n",
    "    y_test = data_test.ix[:, 4]\n",
    "    \n",
    "    y_test_total.extend(y_test)\n",
    "    \n",
    "    ## Logistic Regression\n",
    "    # for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(penalty='l1', tol=0.01)\n",
    "    clf_l1_LR = clf_l1_LR.fit(X_train, y_train)\n",
    "    y_pred_l1 = clf_l1_LR.predict(X_test)\n",
    "    y_pred_l1_total.extend(y_pred_l1)\n",
    "    \n",
    "    clf_l2_LR = LogisticRegression(penalty='l2', tol=0.01)\n",
    "    clf_l2_LR = clf_l2_LR.fit(X_train, y_train)\n",
    "    y_pred_l2 = clf_l2_LR.predict(X_test)\n",
    "    y_pred_l2_total.extend(y_pred_l2)\n",
    "        \n",
    "    ## SVM with unbalanced class weight\n",
    "    clf_SVC = SVC(class_weight = {0:1, 1:3})\n",
    "    clf_SVC = clf_SVC.fit(X_train, y_train)\n",
    "    y_pred_svc = clf_SVC.predict(X_test)\n",
    "    y_pred_svc_total.extend(y_pred_svc)\n",
    "    \n",
    "    ## Decision tree\n",
    "    clf_tree = DecisionTreeClassifier()\n",
    "    clf_tree = clf_tree.fit(X_train, y_train)\n",
    "    y_pred_tree = clf_tree.predict(X_test)\n",
    "    y_pred_tree_total.extend(y_pred_tree)\n",
    "    \n",
    "    \n",
    "    ## GaussianNB\n",
    "    clf_GaussianNB = GaussianNB()\n",
    "    clf_GaussianNB = clf_GaussianNB.fit(X_train, y_train)\n",
    "    y_pred_bayes = clf_GaussianNB.predict(X_test)\n",
    "    y_pred_bayes_total.extend(y_pred_bayes)\n",
    "    \n",
    "\n",
    "print \"L1 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l1_total))\n",
    "\n",
    "print \"L2 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l2_total))\n",
    "\n",
    "print \"SVC\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_svc_total))\n",
    "\n",
    "print \"Decision Tree\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_tree_total))\n",
    "\n",
    "print \"Gaussian Naive Bayes\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_bayes_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction With Global Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression\n",
      "\tPrecision: 0.419\n",
      "\tRecall: 0.224\n",
      "\tF1: 0.292\n",
      "\tAccuracy: 0.685\n",
      "\n",
      "L2 Logistic Regression\n",
      "\tPrecision: 0.300\n",
      "\tRecall: 0.155\n",
      "\tF1: 0.205\n",
      "\tAccuracy: 0.650\n",
      "\n",
      "SVC\n",
      "\tPrecision: 0.000\n",
      "\tRecall: 0.000\n",
      "\tF1: 0.000\n",
      "\tAccuracy: 0.710\n",
      "\n",
      "Decision Tree\n",
      "\tPrecision: 0.218\n",
      "\tRecall: 0.207\n",
      "\tF1: 0.212\n",
      "\tAccuracy: 0.555\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\tPrecision: 0.309\n",
      "\tRecall: 0.362\n",
      "\tF1: 0.333\n",
      "\tAccuracy: 0.580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "## Read data from features_labels_tobii.csv\n",
    "\n",
    "# df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "list_id_report = list(df_merge['id'].unique())\n",
    "\n",
    "y_test_total = []\n",
    "y_pred_l1_total = []\n",
    "y_pred_l2_total = []\n",
    "y_pred_svc_total = []\n",
    "y_pred_tree_total = []\n",
    "y_pred_bayes_total = []\n",
    "y_pred_rf_total = []\n",
    "\n",
    "## Leave-one-out machine learning methods (try Logistic Regression first)\n",
    "for i in range(0, len(list_id_report)):\n",
    "    id_str = list_id_report[i]\n",
    "    \n",
    "    # print id_str\n",
    "    # print \"Run: \" + str(i)\n",
    "    data_train = df_merge.ix[df_merge['id'] != id_str]\n",
    "    # print data_train.shape\n",
    "    data_test = df_merge.ix[df_merge['id'] == id_str]\n",
    "    # print data_test.shape\n",
    "    feature_index_global = [18,19,20,21,22,23,24,25,26,\n",
    "                            42,43,44,45,46,47,48,49,50,\n",
    "                            51,52,53,54,55,56,57,58,59,\n",
    "                            60,61,62,63,64,65,66,67]\n",
    "    X_train = data_train.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_train = data_train.ix[:, 4]\n",
    "    \n",
    "#     sm = SMOTE(random_state=48)\n",
    "#     X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    X_test = data_test.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_test = data_test.ix[:, 4]\n",
    "    \n",
    "    y_test_total.extend(y_test)\n",
    "    \n",
    "    ## Logistic Regression\n",
    "    clf_l1_LR = LogisticRegression(penalty='l1', tol=0.01)\n",
    "    clf_l1_LR = clf_l1_LR.fit(X_train, y_train)\n",
    "    y_pred_l1 = clf_l1_LR.predict(X_test)\n",
    "    y_pred_l1_total.extend(y_pred_l1)\n",
    "    \n",
    "    clf_l2_LR = LogisticRegression(penalty='l2', tol=0.01)\n",
    "    clf_l2_LR = clf_l2_LR.fit(X_train, y_train)\n",
    "    y_pred_l2 = clf_l2_LR.predict(X_test)\n",
    "    y_pred_l2_total.extend(y_pred_l2)\n",
    "        \n",
    "    ## SVM with unbalanced class weight\n",
    "    clf_SVC = SVC(class_weight = {0:1, 1:3})\n",
    "    clf_SVC = clf_SVC.fit(X_train, y_train)\n",
    "    y_pred_svc = clf_SVC.predict(X_test)\n",
    "    y_pred_svc_total.extend(y_pred_svc)\n",
    "    \n",
    "    ## Decision tree\n",
    "    clf_tree = DecisionTreeClassifier()\n",
    "    clf_tree = clf_tree.fit(X_train, y_train)\n",
    "    y_pred_tree = clf_tree.predict(X_test)\n",
    "    y_pred_tree_total.extend(y_pred_tree)\n",
    "    \n",
    "    \n",
    "    ## GaussianNB\n",
    "    clf_GaussianNB = GaussianNB()\n",
    "    clf_GaussianNB = clf_GaussianNB.fit(X_train, y_train)\n",
    "    y_pred_bayes = clf_GaussianNB.predict(X_test)\n",
    "    y_pred_bayes_total.extend(y_pred_bayes)\n",
    "    \n",
    "#     ## Random Forest\n",
    "#     clf_rf = RandomForestClassifier()\n",
    "#     clf_rf = clf_rf.fit(X_train, y_train)\n",
    "#     y_pred_rf = clf_rf.predict(X_test)\n",
    "#     y_pred_rf_total.extend(y_pred_rf)\n",
    "    \n",
    "\n",
    "print \"L1 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l1_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l1_total))\n",
    "\n",
    "print \"L2 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l2_total))\n",
    "\n",
    "print \"SVC\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_svc_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_svc_total))\n",
    "\n",
    "print \"Decision Tree\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_tree_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_tree_total))\n",
    "\n",
    "print \"Gaussian Naive Bayes\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_bayes_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_bayes_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Whether or not predictions can be made equally well for all participants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anon01\n",
      "Run: 0\n",
      "\tPrecision: 0.333\n",
      "\tRecall: 0.800\n",
      "\tF1: 0.471\n",
      "\tAccuracy: 0.438\n",
      "\n",
      "Anon02\n",
      "Run: 1\n",
      "\tPrecision: 0.500\n",
      "\tRecall: 0.429\n",
      "\tF1: 0.462\n",
      "\tAccuracy: 0.588\n",
      "\n",
      "Anon03\n",
      "Run: 2\n",
      "\tPrecision: 0.714\n",
      "\tRecall: 0.833\n",
      "\tF1: 0.769\n",
      "\tAccuracy: 0.800\n",
      "\n",
      "Anon04\n",
      "Run: 3\n",
      "\tPrecision: 0.500\n",
      "\tRecall: 0.250\n",
      "\tF1: 0.333\n",
      "\tAccuracy: 0.733\n",
      "\n",
      "Anon05\n",
      "Run: 4\n",
      "\tPrecision: 0.333\n",
      "\tRecall: 0.250\n",
      "\tF1: 0.286\n",
      "\tAccuracy: 0.667\n",
      "\n",
      "Anon06\n",
      "Run: 5\n",
      "\tPrecision: 0.000\n",
      "\tRecall: 0.000\n",
      "\tF1: 0.000\n",
      "\tAccuracy: 0.733\n",
      "\n",
      "Anon07\n",
      "Run: 6\n",
      "\tPrecision: 0.100\n",
      "\tRecall: 1.000\n",
      "\tF1: 0.182\n",
      "\tAccuracy: 0.357\n",
      "\n",
      "Anon08\n",
      "Run: 7\n",
      "\tPrecision: 0.167\n",
      "\tRecall: 0.500\n",
      "\tF1: 0.250\n",
      "\tAccuracy: 0.600\n",
      "\n",
      "Anon09\n",
      "Run: 8\n",
      "\tPrecision: 0.500\n",
      "\tRecall: 0.111\n",
      "\tF1: 0.182\n",
      "\tAccuracy: 0.438\n",
      "\n",
      "Anon10\n",
      "Run: 9\n",
      "\tPrecision: 0.333\n",
      "\tRecall: 0.800\n",
      "\tF1: 0.471\n",
      "\tAccuracy: 0.438\n",
      "\n",
      "Anon11\n",
      "Run: 10\n",
      "\tPrecision: 1.000\n",
      "\tRecall: 0.556\n",
      "\tF1: 0.714\n",
      "\tAccuracy: 0.750\n",
      "\n",
      "Anon12\n",
      "Run: 11\n",
      "\tPrecision: 0.333\n",
      "\tRecall: 0.750\n",
      "\tF1: 0.462\n",
      "\tAccuracy: 0.533\n",
      "\n",
      "Anon13\n",
      "Run: 12\n",
      "\tPrecision: 0.250\n",
      "\tRecall: 1.000\n",
      "\tF1: 0.400\n",
      "\tAccuracy: 0.800\n",
      "\n",
      "Final Results: L2 Logistic Regression\n",
      "\tPrecision: 0.370\n",
      "\tRecall: 0.517\n",
      "\tF1: 0.432\n",
      "\tAccuracy: 0.605\n",
      "\n",
      "Precision\n",
      "1.0\n",
      "0.0\n",
      "0.38956043956\n",
      "0.333333333333\n",
      "0.252994517262\n",
      "Recall\n",
      "1.0\n",
      "0.0\n",
      "0.55989010989\n",
      "0.555555555556\n",
      "0.321224269986\n",
      "F1\n",
      "0.769230769231\n",
      "0.0\n",
      "0.383111835374\n",
      "0.4\n",
      "0.204591672648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "## Read data from features_labels_tobii.csv\n",
    "\n",
    "# df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "list_id_report = list(df_merge['id'].unique())\n",
    "\n",
    "y_test_total = []\n",
    "y_pred_l2_total = []\n",
    "\n",
    "list_pre = []\n",
    "list_rec = []\n",
    "list_f1 = []\n",
    "\n",
    "## Leave-one-out machine learning methods (try Logistic Regression first)\n",
    "for i in range(0, len(list_id_report)):\n",
    "    id_str = list_id_report[i]\n",
    "    \n",
    "    print id_str\n",
    "    print \"Run: \" + str(i)\n",
    "    data_train = df_merge.ix[df_merge['id'] != id_str]\n",
    "    # print data_train.shape\n",
    "    data_test = df_merge.ix[df_merge['id'] == id_str]\n",
    "    # print data_test.shape\n",
    "    feature_index_global = [18,19,20,21,22,23,24,25,26,\n",
    "                            42,43,44,45,46,47,48,49,50,\n",
    "                            51,52,53,54,55,56,57,58,59,\n",
    "                            60,61,62,63,64,65,66,67]\n",
    "    X_train = data_train.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_train = data_train.ix[:, 4]\n",
    "    \n",
    "    sm = SMOTE(random_state=48)\n",
    "    X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    X_test = data_test.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_test = data_test.ix[:, 4]\n",
    "    \n",
    "    y_test_total.extend(y_test)\n",
    "    \n",
    "    ## Logistic Regression\n",
    "    \n",
    "    clf_l2_LR = LogisticRegression(penalty='l2', tol=0.01)\n",
    "    clf_l2_LR = clf_l2_LR.fit(X_train, y_train)\n",
    "    y_pred_l2 = clf_l2_LR.predict(X_test)\n",
    "    y_pred_l2_total.extend(y_pred_l2)\n",
    "    \n",
    "    print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred_l2))\n",
    "    print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred_l2))\n",
    "    print(\"\\tF1: %1.3f\" % f1_score(y_test, y_pred_l2))\n",
    "    print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test, y_pred_l2))\n",
    "    \n",
    "    list_pre.append(precision_score(y_test, y_pred_l2))\n",
    "    list_rec.append(recall_score(y_test, y_pred_l2))\n",
    "    list_f1.append(f1_score(y_test, y_pred_l2))\n",
    "\n",
    "print \"Final Results: L2 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l2_total))\n",
    "print(\"Precision\")\n",
    "print np.max(list_pre)\n",
    "print np.min(list_pre)\n",
    "print np.mean(list_pre)\n",
    "print np.median(list_pre)\n",
    "print np.std(list_pre)\n",
    "print(\"Recall\")\n",
    "print np.max(list_rec)\n",
    "print np.min(list_rec)\n",
    "print np.mean(list_rec)\n",
    "print np.median(list_rec)\n",
    "print np.std(list_rec)\n",
    "print(\"F1\")\n",
    "print np.max(list_f1)\n",
    "print np.min(list_f1)\n",
    "print np.mean(list_f1)\n",
    "print np.median(list_f1)\n",
    "print np.std(list_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 whether or not predictions can be made well across the entire length of the lecture videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: L2 Logistic Regression\n",
      "\tPrecision: 0.526\n",
      "\tRecall: 0.588\n",
      "\tF1: 0.556\n",
      "\tAccuracy: 0.673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "## Read data from features_labels_tobii.csv\n",
    "\n",
    "# df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "list_id_report = list(df_merge['id'].unique())\n",
    "\n",
    "y_test_total = []\n",
    "y_pred_l2_total = []\n",
    "\n",
    "list_pre = []\n",
    "list_rec = []\n",
    "list_f1 = []\n",
    "\n",
    "## Leave-one-out machine learning methods (try Logistic Regression first)\n",
    "for i in range(0, len(list_id_report)):\n",
    "    id_str = list_id_report[i]\n",
    "    \n",
    "    # data_train = df_merge.ix[df_merge['id'] != id_str]\n",
    "    # data_train = data_train.ix[data_train['video_id'] == \"Solar\"]\n",
    "    # data_train = data_train.ix[data_train['video_id'] != \"Solar\"]\n",
    "    \n",
    "    # data_train = df_merge\n",
    "    # data_train = data_train.ix[data_train['video_id'] != \"Solar\"]\n",
    "    \n",
    "    data_train = df_merge\n",
    "    data_train = data_train.ix[((data_train['video_id'] != \"Solar\")&\n",
    "                                (data_test['endtime_video']/data_test['video_length'] < 0.5))|\n",
    "                               (data_train['video_id'] == \"Solar\")\n",
    "                              ]\n",
    "    \n",
    "    # print data_train.shape\n",
    "    data_test = df_merge.ix[df_merge['id'] == id_str]\n",
    "    data_test = data_test.ix[(data_test['video_id'] != \"Solar\")&(data_test['endtime_video']/data_test['video_length'] >= 0.5)]\n",
    "    # print data_test.shape\n",
    "    feature_index_global = [18,19,20,21,22,23,24,25,26,\n",
    "                            42,43,44,45,46,47,48,49,50,\n",
    "                            51,52,53,54,55,56,57,58,59,\n",
    "                            60,61,62,63,64,65,66,67]\n",
    "    X_train = data_train.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_train = data_train.ix[:, 4]\n",
    "    \n",
    "    sm = SMOTE(random_state=48)\n",
    "    X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    X_test = data_test.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_test = data_test.ix[:, 4]\n",
    "    \n",
    "    y_test_total.extend(y_test)\n",
    "    \n",
    "    ## Logistic Regression\n",
    "    \n",
    "    clf_l2_LR = LogisticRegression(penalty='l2', tol=0.01)\n",
    "    clf_l2_LR = clf_l2_LR.fit(X_train, y_train)\n",
    "    y_pred_l2 = clf_l2_LR.predict(X_test)\n",
    "    y_pred_l2_total.extend(y_pred_l2)\n",
    "    \n",
    "    list_pre.append(precision_score(y_test, y_pred_l2))\n",
    "    list_rec.append(recall_score(y_test, y_pred_l2))\n",
    "    list_f1.append(f1_score(y_test, y_pred_l2))\n",
    "\n",
    "print \"Final Results: L2 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l2_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 whether or not a model trained on one video translates to good predictions in other videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: L2 Logistic Regression\n",
      "\tPrecision: 0.444\n",
      "\tRecall: 0.593\n",
      "\tF1: 0.508\n",
      "\tAccuracy: 0.659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "## Read data from features_labels_tobii.csv\n",
    "\n",
    "# df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "df_merge = pd.read_csv(\"features_labels_tobii.csv\")\n",
    "list_id_report = list(df_merge['id'].unique())\n",
    "\n",
    "y_test_total = []\n",
    "y_pred_l2_total = []\n",
    "\n",
    "list_pre = []\n",
    "list_rec = []\n",
    "list_f1 = []\n",
    "\n",
    "## Leave-one-out machine learning methods (try Logistic Regression first)\n",
    "for i in range(0, len(list_id_report)):\n",
    "    id_str = list_id_report[i]\n",
    "    \n",
    "    # data_train = df_merge.ix[df_merge['id'] != id_str]\n",
    "    # data_train = data_train.ix[data_train['video_id'] == \"Solar\"]\n",
    "    # data_train = data_train.ix[data_train['video_id'] != \"Solar\"]\n",
    "    \n",
    "    # data_train = df_merge\n",
    "    # data_train = data_train.ix[data_train['video_id'] != \"Solar\"]\n",
    "    \n",
    "    data_train = df_merge\n",
    "    data_train = data_train.ix[data_train['video_id'] == \"Solar\"]\n",
    "    \n",
    "    # print data_train.shape\n",
    "    data_test = df_merge.ix[df_merge['id'] == id_str]\n",
    "    # data_test = data_test.ix[data_test['video_id'] == \"Solar\"]\n",
    "    data_test = data_test.ix[data_test['video_id'] != \"Solar\"]\n",
    "    # print data_test.shape\n",
    "    feature_index_global = [18,19,20,21,22,23,24,25,26,\n",
    "                            42,43,44,45,46,47,48,49,50,\n",
    "                            51,52,53,54,55,56,57,58,59,\n",
    "                            60,61,62,63,64,65,66,67]\n",
    "    X_train = data_train.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_train = data_train.ix[:, 4]\n",
    "    \n",
    "    sm = SMOTE(random_state=48)\n",
    "    X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    X_test = data_test.ix[:, feature_index_global].fillna(value=0)\n",
    "    y_test = data_test.ix[:, 4]\n",
    "    \n",
    "    y_test_total.extend(y_test)\n",
    "    \n",
    "    ## Logistic Regression\n",
    "    \n",
    "    clf_l2_LR = LogisticRegression(penalty='l2', tol=0.01)\n",
    "    clf_l2_LR = clf_l2_LR.fit(X_train, y_train)\n",
    "    y_pred_l2 = clf_l2_LR.predict(X_test)\n",
    "    y_pred_l2_total.extend(y_pred_l2)\n",
    "    \n",
    "    list_pre.append(precision_score(y_test, y_pred_l2))\n",
    "    list_rec.append(recall_score(y_test, y_pred_l2))\n",
    "    list_f1.append(f1_score(y_test, y_pred_l2))\n",
    "\n",
    "print \"Final Results: L2 Logistic Regression\"\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test_total, y_pred_l2_total))\n",
    "print(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test_total, y_pred_l2_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
